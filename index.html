---
layout: default
---

<div class="row">
<p>
  Discrete abstractions such as objects, concepts, and events are at the basis of our ability to perceive the world, relate the pieces in it, and reason about their causal structure. The research communities of object-centric representation learning and causal machine learning, have – largely independently – pursued a similar agenda of equipping machine learning models with more structured representations and reasoning capabilities. Despite their different languages, these communities have similar premises and overall pursue the same benefits. They operate under the assumption that, compared to a monolithic/black-box representation, a structured model will improve systematic generalization, robustness to distribution shifts, downstream learning efficiency, and interpretability. Both communities typically approach the problem from opposite directions. Work on causality often assumes a known (true) decomposition into causal factors and is focused on inferring and leveraging interactions between them. Object-centric representation learning, on the other hand, typically starts from an unstructured input and aims to infer a useful decomposition into meaningful factors, and has so far been less concerned with their interactions.
</p>
<p>
  This workshop aims to bring together researchers from object-centric and causal representation learning. To help integrate ideas from these areas, we invite perspectives from the other fields including cognitive psychology and neuroscience. We hope that this creates opportunities for discussion, presenting cutting-edge research, establishing new collaborations and identifying future research directions.
</p>
<p>
  In particular, we welcome contributions in the direction of:
  <ul>
    <li>Benchmarks that quantitatively highlight the benefits of abstract representations</li>
    <li>Methods that extract and/or reason on abstract entities</li>
    <li>Integrating ideas from the causality literature into neural network architectures</li>
    <li>Integrating tools from deep learning into more traditional causal discovery approaches, which may result in a lack of recovery guarantees</li>
    <li>Inference of relations between observed or unobserved variables</li>
    <li>Self-supervised learning of structured representations</li>
    <li>Learning causal world models and structure in acting agents (e.g. in reinforcement learning)</li>
    <li>Leveraging structure for a reinforcement learning agent’s model or policy</li>
    <li>Reasoning tasks, interventional and counterfactual questions</li>
    <li>Theoretical understanding on the challenges of learning abstractions and invariances from data</li>
    <li>Applications (e.g. in computer vision, audio processing, robotics)</li>
  </ul>
</p>

</div>

<div id="organizers" class="row">
<h2>Organizers (alphabetically)</h2>
<div class="break"></div>
<ul>
  <li><a href="https://sungjinahn.com/">Sungjin Ahn</a> (KAIST, Rutgers University)</li>
  <li><a href="http://cogscikid.com/">Wilka Carvalho</a> (University of Michigan)</li>
  <li><a href="https://qwlouse.github.io/">Klaus Greff</a> (Google Brain)</li>
  <li><a href="https://scholar.google.ca/citations?user=hV5D8GYAAAAJ&hl=en">Tong He</a> (Amazon AWS)</li>
  <li><a href="http://tkipf.github.io/">Thomas Kipf</a> (Google Brain)</li>
  <li><a href="https://www.is.mpg.de/~flocatello">Francesco Locatello</a> (Amazon AWS)</li>
  <li><a href="https://loewex.github.io/">Sindy Löwe</a> (University of Amsterdam)</li>
</ul>
</div>

<div id="program-committee" class="row">
<h2>Program Committee</h2>
<table>
  <tbody>
  <tr><td>TBD</td><td>TBD</td><td>TBD</td></tr>
  </tbody>
</table>
</div>
